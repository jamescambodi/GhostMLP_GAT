{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import PointNet++ utility functions\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N, dtype=torch.float32).to(device) * 1e10  # Ensure Float type\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(npoint):\n",
    "        centroids[:, i] = farthest\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1).float()  # Ensure dist is Float type\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx\n",
    "\n",
    "\n",
    "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        radius:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
    "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
    "    torch.cuda.empty_cache()\n",
    "    new_xyz = index_points(xyz, fps_idx)\n",
    "    torch.cuda.empty_cache()\n",
    "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "    torch.cuda.empty_cache()\n",
    "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
    "    torch.cuda.empty_cache()\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = index_points(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "    if returnfps:\n",
    "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
    "    else:\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "def sample_and_group_all(xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, 3]\n",
    "        new_points: sampled points data, [B, 1, N, 3+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
    "        # new_xyz: sampled points position data, [B, npoint, C]\n",
    "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "\n",
    "        new_points = torch.max(new_points, 2)[0]\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstractionMsg(nn.Module):\n",
    "    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n",
    "        super(PointNetSetAbstractionMsg, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius_list = radius_list\n",
    "        self.nsample_list = nsample_list\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        self.bn_blocks = nn.ModuleList()\n",
    "        for i in range(len(mlp_list)):\n",
    "            convs = nn.ModuleList()\n",
    "            bns = nn.ModuleList()\n",
    "            last_channel = in_channel + 3\n",
    "            for out_channel in mlp_list[i]:\n",
    "                convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "                bns.append(nn.BatchNorm2d(out_channel))\n",
    "                last_channel = out_channel\n",
    "            self.conv_blocks.append(convs)\n",
    "            self.bn_blocks.append(bns)\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        B, N, C = xyz.shape\n",
    "        S = self.npoint\n",
    "\n",
    "        new_xyz = index_points(xyz, farthest_point_sample(xyz, S))\n",
    "        new_points_list = []\n",
    "        for i, radius in enumerate(self.radius_list):\n",
    "            K = self.nsample_list[i]\n",
    "            group_idx = query_ball_point(radius, K, xyz, new_xyz)\n",
    "            grouped_xyz = index_points(xyz, group_idx)\n",
    "            grouped_xyz -= new_xyz.view(B, S, 1, C)\n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, group_idx)\n",
    "                grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1)\n",
    "            else:\n",
    "                grouped_points = grouped_xyz\n",
    "\n",
    "            grouped_points = grouped_points.permute(0, 3, 2, 1)  # [B, D, K, S]\n",
    "            for j in range(len(self.conv_blocks[i])):\n",
    "                conv = self.conv_blocks[i][j]\n",
    "                bn = self.bn_blocks[i][j]\n",
    "                grouped_points =  F.relu(bn(conv(grouped_points)))\n",
    "            new_points = torch.max(grouped_points, 2)[0]  # [B, D', S]\n",
    "            new_points_list.append(new_points)\n",
    "\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        new_points_concat = torch.cat(new_points_list, dim=1)\n",
    "        return new_xyz, new_points_concat\n",
    "\n",
    "        \n",
    "# Define GhostModule\n",
    "class GhostModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ratio=2, kernel_size=1, dw_size=3, stride=1, relu=True):\n",
    "        super(GhostModule, self).__init__()\n",
    "        init_channels = max(1, int(out_channels / ratio))\n",
    "        cheap_channels = out_channels - init_channels\n",
    "\n",
    "        self.primary_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, init_channels, kernel_size, stride, kernel_size // 2, bias=False),\n",
    "            nn.BatchNorm1d(init_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Identity(),\n",
    "        )\n",
    "\n",
    "        self.cheap_operation = nn.Sequential(\n",
    "            nn.Conv1d(init_channels, cheap_channels, dw_size, stride=1, padding=dw_size // 2, groups=init_channels, bias=False),\n",
    "            nn.BatchNorm1d(cheap_channels),\n",
    "            nn.ReLU(inplace=True) if relu else nn.Identity(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.primary_conv(x)\n",
    "        x2 = self.cheap_operation(x1)\n",
    "\n",
    "        if x2.shape[1] < x1.shape[1]:  # Handle edge cases\n",
    "            padding = x1.shape[1] - x2.shape[1]\n",
    "            x2 = F.pad(x2, (0, 0, 0, padding))\n",
    "\n",
    "        out = torch.cat([x1, x2], dim=1)\n",
    "        return out[:, :x.shape[1], :]  # Ensure output shape matches\n",
    "\n",
    "\n",
    "# Define GhostMLPModel\n",
    "class GhostMLPModel(nn.Module):\n",
    "    def __init__(self, num_class, normal_channel=False):\n",
    "        super(GhostMLPModel, self).__init__()\n",
    "        in_channel = 3 if normal_channel else 0\n",
    "\n",
    "        self.normal_channel = normal_channel\n",
    "        self.sa1 = PointNetSetAbstractionMsg(512, [0.1, 0.2, 0.4], [16, 32, 128], in_channel, [[32, 32, 64], [64, 64, 128], [64, 96, 128]])\n",
    "        self.sa2 = PointNetSetAbstractionMsg(128, [0.2, 0.4, 0.8], [32, 64, 128], 320, [[64, 64, 128], [128, 128, 256], [128, 128, 256]])\n",
    "        self.sa3 = PointNetSetAbstraction(None, None, None, 640 + 3, [256, 512, 1024], True)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            GhostModule(1024, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            GhostModule(512, 256),\n",
    "            nn.Dropout(0.5),\n",
    "            GhostModule(256, 256),  # Project features to 3 dimensions for compatibility with the mask\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(256, num_class)  # Classification layer\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        B, _, _ = xyz.shape\n",
    "        if self.normal_channel:\n",
    "            norm = xyz[:, 3:, :]\n",
    "            xyz = xyz[:, :3, :]\n",
    "        else:\n",
    "            norm = None\n",
    "\n",
    "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "\n",
    "        # Flatten the global features\n",
    "        x = l3_points.view(B, 1024)\n",
    "\n",
    "        # Feature encoding\n",
    "        features = self.encoder(x.unsqueeze(-1)).squeeze(-1)  # [B, 3] after projection\n",
    "\n",
    "        # Final classification output\n",
    "        logits = F.log_softmax(self.fc3(features), dim=-1)\n",
    "\n",
    "        return logits, l3_points, features  # Output logits, global features, and reduced features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_idx shape: torch.Size([1, 512, 16])\n"
     ]
    }
   ],
   "source": [
    "B, N, S, C = 1, 1024, 512, 3\n",
    "radius = 0.2\n",
    "nsample = 16\n",
    "\n",
    "xyz = torch.rand(B, N, C).cuda()  # All points\n",
    "new_xyz = torch.rand(B, S, C).cuda()  # Query points\n",
    "\n",
    "group_idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "print(\"group_idx shape:\", group_idx.shape)  # Should be [B, S, nsample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/5739 [00:00<?, ?batch/s]C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1488\\3943462930.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  points = torch.tensor(points, dtype=torch.float32).permute(0, 2, 1)  # [B, 3, N]\n",
      "Epoch 1/10: 100%|██████████| 5739/5739 [22:02:36<00:00, 13.83s/batch, accuracy=74.5, loss=0.207]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 4823.9940, Accuracy: 74.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  25%|██▍       | 1424/5739 [3:39:40<11:16:02,  9.40s/batch, accuracy=82.9, loss=0.905]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import SinglePoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Normalize point cloud\n",
    "def pc_normalize(pc):\n",
    "    centroid = pc.mean(axis=0)\n",
    "    pc -= centroid\n",
    "    m = (pc**2).sum(axis=1).max()**0.5\n",
    "    pc /= m\n",
    "    return pc\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    points = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    points = [pc_normalize(point_set) for point_set in points]\n",
    "    points = torch.tensor(points, dtype=torch.float32).permute(0, 2, 1)  # [B, 3, N]\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return points, labels\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            for points, labels in tepoch:\n",
    "                points, labels = points.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                logits, _, _ = model(points)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                _, predicted = logits.max(1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=100.0 * correct / total)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {100.0 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    num_classes = 40  # Based on the classnames in the dataloader\n",
    "    num_epochs = 10\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    save_path = \"ghostmlp_model.pth\"\n",
    "    dataset_root = \"../data/modelnetdata/*/train\"  # Update this to your dataset path\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize dataset and dataloader\n",
    "    train_dataset = SinglePoint(dataset_root)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Initialize model, loss, optimizer\n",
    "    model = GhostMLPModel(num_class=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    train(model, train_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the .xyz file is: (1024, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "__main__.GhostMLPModel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_xyz_shape(file_path):\n",
    "    # Load the .xyz file into a NumPy array\n",
    "    data = np.loadtxt(file_path)\n",
    "\n",
    "    # Return the shape of the data\n",
    "    return data.shape\n",
    "\n",
    "# Example usage\n",
    "file_path = \"../data/modelnetdata/airplane/train/airplane_0001_001.xyz\"\n",
    "shape = get_xyz_shape(file_path)\n",
    "print(f\"The shape of the .xyz file is: {shape}\")\n",
    "GhostMLPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Train Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/183640 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from dataloader import SinglePoint\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-bs\", \"--batchSize\", type=int, default=1)\n",
    "parser.add_argument(\"-num_class\", type=int, default=40)\n",
    "parser.add_argument('--val_path', default='../data/modelnetdata/*/test', help='Path to the test data')\n",
    "parser.add_argument('--train_path', default='../data/modelnetdata/*/train', help='Path to the train data')\n",
    "parser.add_argument('--output_data_path', default='../data/modelnet_trained_feature/', help='Output feature directory')\n",
    "parser.add_argument(\"--workers\", type=int, default=4, help='Number of DataLoader workers')\n",
    "parser.set_defaults(train=False)\n",
    "\n",
    "\n",
    "def save_features(features, scene_name, file_name, output_dir, split_name):\n",
    "    \"\"\"Save features to a specified directory.\"\"\"\n",
    "    out_file = os.path.join(output_dir, scene_name, split_name)\n",
    "    os.makedirs(out_file, exist_ok=True)\n",
    "    file_path = os.path.join(out_file, file_name + '.pth')\n",
    "    torch.save(features.cpu(), file_path)\n",
    "\n",
    "\n",
    "    # Tracking metrics\n",
    "def evaluate(loader, split_name):\n",
    "    \"\"\"Evaluate the model on a dataset (train or val).\"\"\"\n",
    "    mean_correct = []\n",
    "    class_acc = np.zeros((args.num_class, 3))\n",
    "    output_dir = args.output_data_path\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, total=len(loader)):\n",
    "            points, target, scene_name, file_name = data[0], data[1], data[2], data[3]\n",
    "\n",
    "            # Debugging: Check the shape of points\n",
    "            print(f\"Points shape before transpose: {points.shape}\")\n",
    "\n",
    "            # Ensure points is a tensor and has the expected shape\n",
    "            if not isinstance(points, torch.Tensor):\n",
    "                points = torch.tensor(points)\n",
    "\n",
    "            # Preprocess points\n",
    "            points = points.transpose(1, 2).float().cuda()  # Swap dimensions 1 and 2\n",
    "            target = target.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            pred, _, features = model(points)\n",
    "\n",
    "            # Save features with the explicit split_name\n",
    "            file_name_no_ext = os.path.splitext(os.path.basename(file_name[-1]))[0]\n",
    "            save_features(features, scene_name[-1], file_name_no_ext, output_dir, split_name)\n",
    "\n",
    "            # Voting mechanism\n",
    "            vote_pool = torch.zeros(target.shape[0], args.num_class).cuda()\n",
    "            vote_pool += pred\n",
    "            pred_choice = vote_pool.data.max(1)[1]\n",
    "\n",
    "            # Accuracy calculation\n",
    "            for cat in np.unique(target.cpu()):\n",
    "                class_acc[cat, 0] += pred_choice[target == cat].eq(target[target == cat].long().data).cpu().sum().item()\n",
    "                class_acc[cat, 1] += target[target == cat].shape[0]\n",
    "            correct = pred_choice.eq(target.long().data).cpu().sum().item()\n",
    "            mean_correct.append(correct / points.size(0))\n",
    "\n",
    "        class_acc[:, 2] = class_acc[:, 0] / class_acc[:, 1]\n",
    "        instance_acc = np.mean(mean_correct)\n",
    "        mean_class_acc = np.mean(class_acc[:, 2])\n",
    "\n",
    "        print(f\"{split_name} Instance Accuracy: {instance_acc:.4f}, Class Accuracy: {mean_class_acc:.4f}\")\n",
    "        return instance_acc, mean_class_acc\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args, _  = parser.parse_known_args()\n",
    "\n",
    "    # Load datasets and dataloaders\n",
    "    train_dataset = SinglePoint(args.train_path)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=False, batch_size=args.batchSize, num_workers=args.workers)\n",
    "    val_dataset = SinglePoint(args.val_path)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=args.batchSize, num_workers=args.workers)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = GhostMLPModel(args.num_class).cuda()\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate on train and val sets\n",
    "    print(\"Evaluating on Train Set...\")\n",
    "    evaluate(train_loader, \"Train\")\n",
    "    print(\"Evaluating on Validation Set...\")\n",
    "    evaluate(val_loader, \"Validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of the .pth file:\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7138e-03, 0.0000e+00, 6.1926e-04,\n",
      "         1.8625e-03, 1.9308e-04, 8.1920e-04, 1.2489e-03, 0.0000e+00, 9.2528e-04,\n",
      "         1.7765e-03, 3.5468e-04, 0.0000e+00, 0.0000e+00, 8.5227e-04, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3547e-04,\n",
      "         0.0000e+00, 1.0643e-04, 6.6540e-04, 9.4780e-05, 4.3867e-04, 3.0095e-04,\n",
      "         0.0000e+00, 3.9032e-04, 0.0000e+00, 4.9707e-05, 0.0000e+00, 0.0000e+00,\n",
      "         1.3510e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9228\\2853942481.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Replace 'file_path' with the path to your .pth file\n",
    "file_path = \"../data/modelnet_trained_feature/airplane/Train/airplane_0001_001.pth\"\n",
    "\n",
    "# Load and print the entire content of the .pth file\n",
    "try:\n",
    "    features = torch.load(file_path)\n",
    "    \n",
    "    # If it's a dictionary, print keys and their associated values\n",
    "    if isinstance(features, dict):\n",
    "        print(\"Content of the .pth file:\")\n",
    "        for key, value in features.items():\n",
    "            print(f\"{key}:\")\n",
    "            print(value)\n",
    "    else:\n",
    "        # Print directly if it's not a dictionary\n",
    "        print(\"Content of the .pth file:\")\n",
    "        print(features)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading the .pth file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1, 512)\n",
      "Cannot reduce to 2 components. Using n_components=1 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18056\\1352075715.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(\"../data/modelnet_trained_feature/airplane/Train/airplane_0001_001.pth\")\n",
      "c:\\Users\\User\\Anaconda3\\envs\\ghost\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:591: RuntimeWarning: invalid value encountered in divide\n",
      "  explained_variance_ = (S**2) / (n_samples - 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load the .pth file\n",
    "features = torch.load(\"../data/modelnet_trained_feature/airplane/Train/airplane_0001_001.pth\")\n",
    "\n",
    "# Convert tensor to NumPy for visualization\n",
    "# Convert tensor to NumPy for visualization\n",
    "features_np = (\n",
    "    features.cpu().numpy() if isinstance(features, torch.Tensor) \n",
    "    else features['features'].cpu().numpy()\n",
    ")\n",
    "\n",
    "# Check the shape of features\n",
    "print(f\"Features shape: {features_np.shape}\")\n",
    "\n",
    "# Ensure n_components does not exceed min(n_samples, n_features)\n",
    "n_samples, n_features = features_np.shape[0], features_np.shape[1] if len(features_np.shape) > 1 else 1\n",
    "n_components = min(2, n_samples, n_features)\n",
    "\n",
    "if n_components < 2:\n",
    "    print(f\"Cannot reduce to 2 components. Using n_components={n_components} instead.\")\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "features_2d = pca.fit_transform(features_np)\n",
    "\n",
    "# Plot the reduced features\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1] if n_components == 2 else [0] * len(features_2d), alpha=0.5)\n",
    "plt.title(\"Feature Visualization (PCA)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\" if n_components == 2 else \"Constant\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification below with another version of GhostMLP with classification component added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GhostMLP(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim):\n",
    "        super(GhostMLP, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        self.classifier = nn.Linear(latent_dim, output_dim)  # Classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_features = self.encoder(x)  # Extracted features\n",
    "        class_scores = self.classifier(latent_features)  # Class scores\n",
    "        return latent_features, class_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the extended model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GhostMLP(input_dim=1024, latent_dim=64, output_dim=3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 286\n",
      "Input shape before reshaping: torch.Size([128, 20, 1024, 64])\n",
      "Labels shape: torch.Size([128])\n",
      "Input shape after reshaping: torch.Size([128, 1310720])\n",
      "Outputs shape: torch.Size([128, 3])\n",
      "Preds shape: torch.Size([128])\n",
      "Input shape before reshaping: torch.Size([128, 20, 1024, 64])\n",
      "Labels shape: torch.Size([128])\n",
      "Input shape after reshaping: torch.Size([128, 1310720])\n",
      "Outputs shape: torch.Size([128, 3])\n",
      "Preds shape: torch.Size([128])\n",
      "Input shape before reshaping: torch.Size([30, 20, 1024, 64])\n",
      "Labels shape: torch.Size([30])\n",
      "Input shape after reshaping: torch.Size([30, 1310720])\n",
      "Outputs shape: torch.Size([30, 3])\n",
      "Preds shape: torch.Size([30])\n",
      "Validation Mean Class Accuracy: 0.3333\n",
      "Validation Overall Accuracy: 0.3007\n",
      "Class Accuracy: [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import MultiviewPoint  # Updated dataset class\n",
    "\n",
    "def seed_torch(seed=9990):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def evaluate_model(model, val_loader, device, num_classes=40):\n",
    "    model.eval()\n",
    "    all_correct_points = 0\n",
    "    all_points = 0\n",
    "    wrong_class = np.zeros(num_classes)\n",
    "    samples_class = np.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Ensure inputs are Float\n",
    "            inputs = inputs.float()\n",
    "            print(f\"Input shape before reshaping: {inputs.shape}\")  # Debug\n",
    "            print(f\"Labels shape: {labels.shape}\")  # Debug\n",
    "\n",
    "            # Flatten last two dimensions and reshape for the model\n",
    "            batch_size, num_views, feature_dim1, feature_dim2 = inputs.shape\n",
    "            inputs = inputs.view(batch_size, num_views, -1)  # Flatten last two dimensions\n",
    "            inputs = inputs.view(batch_size, -1)  # Combine all views\n",
    "            print(f\"Input shape after reshaping: {inputs.shape}\")  # Debug\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Outputs could be a tuple\n",
    "            if isinstance(outputs, tuple):\n",
    "                _, class_scores = outputs  # Unpack the tuple\n",
    "            else:\n",
    "                class_scores = outputs  # Single output tensor\n",
    "\n",
    "            print(f\"Outputs shape: {class_scores.shape}\")  # Debug\n",
    "\n",
    "            # Predictions\n",
    "            preds = torch.argmax(class_scores, dim=-1)\n",
    "            print(f\"Preds shape: {preds.shape}\")  # Debug\n",
    "\n",
    "            # Calculate results\n",
    "            results = preds == labels\n",
    "            correct_points = torch.sum(results.long())\n",
    "            all_correct_points += correct_points.item()\n",
    "            all_points += results.size(0)\n",
    "\n",
    "            for i in range(results.size(0)):\n",
    "                label = labels[i].item()\n",
    "                samples_class[label] += 1\n",
    "                if not results[i].item():\n",
    "                    wrong_class[label] += 1\n",
    "\n",
    "    # Calculate accuracies\n",
    "    class_acc = (samples_class - wrong_class) / samples_class\n",
    "    val_mean_class_acc = np.mean(class_acc)\n",
    "    val_overall_acc = all_correct_points / all_points\n",
    "\n",
    "    print(f\"Validation Mean Class Accuracy: {val_mean_class_acc:.4f}\")\n",
    "    print(f\"Validation Overall Accuracy: {val_overall_acc:.4f}\")\n",
    "    print(f\"Class Accuracy: {class_acc}\")\n",
    "    return val_overall_acc, val_mean_class_acc\n",
    "\n",
    "\n",
    "\n",
    "# Manually set arguments in Jupyter Notebook\n",
    "args = argparse.Namespace(\n",
    "    name=\"log\",\n",
    "    batch_size=128,\n",
    "    num_classes=3,\n",
    "    ##model_checkpoint=\"../checkpoints/ghostmlp_model.pth\",  # Adjust the checkpoint path\n",
    "    val_path=\"../data/modelnet_features/*/test\",\n",
    "    workers=4,\n",
    ")\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed_torch()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = MultiviewPoint(args.val_path)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize model and load checkpoint\n",
    "model = GhostMLP(input_dim=20 * 1024 * 64, latent_dim=64, output_dim=args.num_classes).to(device)\n",
    "##model.load_state_dict(torch.load(args.model_checkpoint, map_location=device))\n",
    "##print(f\"Model loaded from {args.model_checkpoint}\")\n",
    "\n",
    "# Evaluate the model\n",
    "val_overall_acc, val_mean_class_acc = evaluate_model(model, val_loader, device, num_classes=args.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 286\n",
      "Input shape: torch.Size([128])\n",
      "Labels shape: torch.Size([128, 20, 1024, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 1310720 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 88\u001b[0m\n\u001b[0;32m     83\u001b[0m model \u001b[38;5;241m=\u001b[39m GhostMLP(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m##model.load_state_dict(torch.load(args.model_checkpoint, map_location=device))\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m##print(f\"Model loaded from {args.model_checkpoint}\")\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m val_overall_acc, val_mean_class_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, val_loader, device, num_classes)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(results\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m---> 46\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     samples_class[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results[i]\u001b[38;5;241m.\u001b[39mitem():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 1310720 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import MultiviewPoint  # Your dataset class\n",
    "\n",
    "# Rest of the script\n",
    "\n",
    "\n",
    "def seed_torch(seed=9990):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def evaluate_model(model, val_loader, device, num_classes=40):\n",
    "    model.eval()\n",
    "    all_correct_points = 0\n",
    "    all_points = 0\n",
    "    wrong_class = np.zeros(num_classes)\n",
    "    samples_class = np.zeros(num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Ensure inputs are Float\n",
    "            inputs = inputs.float()\n",
    "            print(f\"Input shape: {inputs.shape}\")\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, class_scores = outputs  # Unpack the tuple\n",
    "            preds = torch.argmax(class_scores, dim=-1)\n",
    "\n",
    "\n",
    "            results = preds == labels\n",
    "            correct_points = torch.sum(results.long())\n",
    "            all_correct_points += correct_points.item()\n",
    "            all_points += results.size(0)\n",
    "            print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "            for i in range(results.size(0)):\n",
    "                label = labels[i].item()\n",
    "                samples_class[label] += 1\n",
    "                if not results[i].item():\n",
    "                    wrong_class[label] += 1\n",
    "\n",
    "    class_acc = (samples_class - wrong_class) / samples_class\n",
    "    val_mean_class_acc = np.mean(class_acc)\n",
    "    val_overall_acc = all_correct_points / all_points\n",
    "\n",
    "    print(f\"Validation Mean Class Accuracy: {val_mean_class_acc:.4f}\")\n",
    "    print(f\"Validation Overall Accuracy: {val_overall_acc:.4f}\")\n",
    "    print(f\"Class Accuracy: {class_acc}\")\n",
    "    return val_overall_acc, val_mean_class_acc\n",
    "\n",
    "\n",
    "# Manually set arguments in Jupyter Notebook\n",
    "args = argparse.Namespace(\n",
    "    name=\"log\",\n",
    "    batch_size=128,\n",
    "    num_classes=3,\n",
    "    ##model_checkpoint=\"../checkpoints/ghostmlp_model.pth\",  # Adjust the checkpoint path\n",
    "    val_path=\"../data/modelnet_features/*/test\",\n",
    "    workers=4,\n",
    ")\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed_torch()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = MultiviewPoint(args.val_path)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize model and load checkpoint\n",
    "model = GhostMLP(input_dim=128, latent_dim=64, output_dim=args.num_classes).to(device)\n",
    "##model.load_state_dict(torch.load(args.model_checkpoint, map_location=device))\n",
    "##print(f\"Model loaded from {args.model_checkpoint}\")\n",
    "\n",
    "# Evaluate the model\n",
    "val_overall_acc, val_mean_class_acc = evaluate_model(model, val_loader, device, num_classes=args.num_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
